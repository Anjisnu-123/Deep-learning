# Common activation function in deep learning

### Step function
- intor
- mathematical formula
- Geometric interpretation
- what makes it useful as an activation function
- limitations

### Sigmoid/Logistic
- intro
- mathematical formula
- geometric interpretation

### Softmax activation

### Hyperbolic tangent/ TanH
- Introduction
- Mathematical formulation
- Geometric interpretation
- Key feature of tanh curve
- What makes it useful as an activation function
- limitations
- when to use tanh

### ReLU
- inrtroduction
- Mathematical formula
- Geometric intepretation
- what makes it useful as an activation function
- Limitations
- what makes it useful
- when to use relu

### Softplus
- Introduction
- Mathematical formula
- Geometric interpretation
- more...

### Leaky relu
- inrtroduction
- Mathematical formula
- Geometric intepretation
- what makes it useful as an activation function
- Limitations
- what makes it useful
- when to use leakyrelu

### Prerelu
- inrtroduction
- Mathematical formula
- Geometric intepretation
- what makes it useful as an activation function
- Limitations
- what makes it useful
- when to use prerelu

### ELU
- inrtroduction
- Mathematical formula
- Geometric intepretation
- what makes it useful as an activation function
- Limitations
- what makes it useful
- when to use elu

### SELU
- inrtroduction
- Mathematical formula
- Geometric intepretation
- what makes it useful as an activation function
- Limitations
- what makes it useful
- when to use relu

### Swish
- inrtroduction
- Mathematical formula
- Geometric intepretation
- what makes it useful as an activation function
- Limitations
- what makes it useful
- when to use swish

### GELU
- inrtroduction
- Mathematical formula
- Geometric intepretation
- what makes it useful as an activation function
- Limitations

### MAXout
- inrtroduction
- Mathematical formula
- Geometric intepretation
- what makes it useful as an activation function
- Limitations
- what makes it useful
- when to use GELU

### Practical usage Guide

### MISH

### SNAKE

### Vanishing and exploring gradients
- intro
- Context and why these problem occurs
  - Vanishsing gradients
  - exploring gradients
    - why do this happen?
    - Solutions?
- overfitting and underfitting
  - Analogy
  - singns
  - solution
- Bias variance trade-off
- regularization techniques
  - L1
  - L2
  - Elasticnet
  - Dropout
  - Early stoppping
  - Data augmentation
  - Early stopping
  - Image augmentation
- Key evaluation metrics
